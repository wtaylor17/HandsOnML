# -*- coding: utf-8 -*-
"""mnist_demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wj0Cudn39ubTk0mqJQmGxPNVfKrfs438
"""

from scipy import io
mnist = io.loadmat('mnist-original.mat')

X_ = np.transpose(mnist['data'])

n_inputs = 28*28
n_hidden1 = 500
n_hidden2 = 500
n_hidden3 = 20 # dimension of latent space
n_hidden4 = n_hidden2
n_hidden5 = n_hidden1
n_outputs = n_inputs

learning_rate = 0.001

import tensorflow as tf
from tensorflow.contrib.layers import fully_connected

with tf.contrib.framework.arg_scope([fully_connected], activation_fn=tf.nn.elu,
                                   weights_initializer=tf.initializers.variance_scaling()):
  X = tf.placeholder(tf.float32, [None, n_inputs])
  
  hidden1 = fully_connected(X, n_hidden1)
  hidden2 = fully_connected(hidden1, n_hidden2)
  hidden3_mean = fully_connected(hidden2, n_hidden3, activation_fn=None)
  hidden3_gamma = fully_connected(hidden2, n_hidden3, activation_fn=None)
  hidden3_sigma = tf.exp(0.5 * hidden3_gamma)
  noise = tf.random_normal(tf.shape(hidden3_sigma), dtype=tf.float32)
  hidden3 = hidden3_mean + hidden3_sigma * noise
  hidden4 = fully_connected(hidden3, n_hidden4)
  hidden5 = fully_connected(hidden4, n_hidden5)
  logits = fully_connected(hidden5, n_outputs)
  outputs = tf.sigmoid(logits)
  
reconstruction_loss = tf.reduce_sum(
             tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits))
latent_loss = 0.5 * tf.reduce_sum(
    tf.exp(hidden3_gamma) + tf.square(hidden3_mean) - 1 - hidden3_gamma)

cost = reconstruction_loss + latent_loss

optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(cost)

init = tf.global_variables_initializer()

n_digits = 60
n_epochs = 1000
batch_size = 150
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets('/tmp/data/')

import numpy as np
import matplotlib
import matplotlib.pyplot as plt

with tf.Session() as sess:
  init.run()
  for epoch in range(n_epochs):
    n_batches = mnist.train.num_examples // batch_size
    for iteration in range(n_batches):
      if epoch % 50 == 0 and iteration % n_batches / 2 == 0:
        print(epoch, 'Cost:', cost.eval(feed_dict={X: }))
      xb, yb = mnist.train.next_batch(batch_size)
      sess.run(training_op, feed_dict={X: xb})
  
  codings_rnd = np.random.normal(size=[n_digits, n_hidden3])
  outputs_val = outputs.eval(feed_dict={hidden3: codings_rnd})
  
  for iteration in range(n_digits):
    plt.imshow(outputs_val[iteration].reshape((28, 28)), cmap=matplotlib.cm.binary,
				interpolation='nearest')
    plt.axis('off')
    plt.show()